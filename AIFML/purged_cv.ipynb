{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only gives realistic (and good to visualize) close price for obs <= 000\n",
    "def generate_test_data(obs, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    returns = pd.DataFrame(np.random.normal(0.002, 0.1, obs), index=pd.date_range('2015-01-02',periods=obs))\n",
    "    close = returns.add(1).cumprod()\n",
    "    close.loc[pd.to_datetime('2015-01-01')] = 1\n",
    "    close = close.sort_index()\n",
    "    close.rename(columns={0:'Close'}, inplace=True)\n",
    "    return close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to reduce leakage is to purge from the training set all observations whose labels overlapped in time with those labels included in the testing set.\n",
    "\n",
    "A label $Y_i = f[[t_{i,0}, t_{i,1}]]$ overlaps with $Y_j$ if any of the three sufficient conditions is met:\n",
    "\n",
    "1. $t_{j,0} \\leq t_{i,0} \\leq t_{j,1} $\n",
    "2. $t_{j,0} \\leq t_{i,1} \\leq t_{j,1} $\n",
    "3. $t_{i,0} \\leq t_{j,0} \\leq t_{j,1} \\leq t_{i,1} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTimes(t1: pd.Series, testTimes):\n",
    "    '''\n",
    "    Given testTimes, find the times of the training observations.\n",
    "    — t1.index: Time when the observation started.\n",
    "    — t1.value: Time when the observation ended.\n",
    "    — testTimes: Times of testing observations.\n",
    "    '''\n",
    "    trn = t1.copy(deep=True)\n",
    "    for i,j in testTimes.items():\n",
    "        df0 = trn[(i<=trn.index)&(trn.index<=j)].index # train starts within test\n",
    "        df1 = trn[(i<=trn)&(trn<=j)].index # train ends within test\n",
    "        df2 = trn[(trn.index<=i)&(j<=trn)].index # train envelops test\n",
    "        trn = trn.drop(df0.union(df1).union(df2))\n",
    "    return trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = generate_test_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 30\n",
    "l = d.index[:-days] + pd.Timedelta(days=days) # index of labels, which are derived from observations that lay x days ahead\n",
    "t1 = pd.Series(l, index=d.index[:-days]) # index: when observation started, value: when observation ended\n",
    "testfrac = 0.2\n",
    "randTestStart = np.random.choice(t1.index[:-int(d.shape[0]*testfrac)])\n",
    "testPeriod = pd.Series({\n",
    "    randTestStart : randTestStart + pd.Timedelta(days=int(d.shape[0]*testfrac))\n",
    "    })\n",
    "trainTimes = getTrainTimes(t1, testPeriod)\n",
    "trainEnd = max(trainTimes.index[testPeriod.index[0] >= trainTimes.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.max(d.values)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=d.index, y=d.Close, name='All Samples'))\n",
    "\n",
    "# lines\n",
    "fig.add_shape(x0=testPeriod.index[-1], x1=testPeriod.index[-1], y0=0, y1=top, line=dict(color='black', width=1, dash='dot'))\n",
    "fig.add_annotation(x=testPeriod.index[-1], y=top, text='Test Start', showarrow=False, yshift=10)\n",
    "\n",
    "fig.add_shape(x0=testPeriod.iloc[-1], x1=testPeriod.iloc[-1], y0=0, y1=top, line=dict(color='black', width=1, dash='dot'))\n",
    "fig.add_annotation(x=testPeriod.iloc[-1], y=top, text='Test End', showarrow=False, yshift=10)\n",
    "\n",
    "fig.add_shape(x0=trainEnd, x1=trainEnd, y0=0, y1=top, line=dict(color='black', width=1, dash='dot'))\n",
    "fig.add_annotation(x=trainEnd, y=top, text='Train End', showarrow=False, yshift=10)\n",
    "\n",
    "# train set\n",
    "fig.add_trace(go.Scatter(x=trainTimes.index, y=d.loc[trainTimes.index].Close, name='Train Samples', mode='markers', marker=dict(color='black', size=4)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Train and Test Split with Purging (In this Example all labels are derived from future observations that lay {days} days ahead)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Close',\n",
    "    width=1400,\n",
    "    height=500,\n",
    "    xaxis_rangeslider_visible=True,\n",
    "    legend_title='Set',\n",
    "    font=dict(\n",
    "        # family=\"Arial\",\n",
    "        size=12,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purged K Fold Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedKFold(KFold):\n",
    "    '''\n",
    "    Extend KFold to work with labels that span intervals\n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training examples in between\n",
    "    '''\n",
    "    def __init__(self,n_splits=3,t1=None,pctEmbargo=0.):\n",
    "        if not isinstance(t1,pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pandas series')\n",
    "        super(PurgedKFold,self).__init__(n_splits,shuffle=False,random_state=None)\n",
    "        self.t1=t1\n",
    "        self.pctEmbargo=pctEmbargo\n",
    "\n",
    "    def split(self,X,y=None,groups=None):\n",
    "        if (X.index==self.t1.index).sum()!=len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "        indices=np.arange(X.shape[0])\n",
    "        mbrg=int(X.shape[0]*self.pctEmbargo)\n",
    "        test_starts=[(i[0],i[-1]+1) for i in np.array_split(np.arange(X.shape[0]),self.n_splits)]\n",
    "        for i,j in test_starts:\n",
    "            t0=self.t1.index[i] # start of test set\n",
    "            test_indices=indices[i:j]\n",
    "            maxT1Idx=self.t1.index.searchsorted(self.t1[test_indices].max())\n",
    "            train_indices=self.t1.index.searchsorted(self.t1[self.t1<=t0].index)\n",
    "            train_indices=np.concatenate((train_indices,indices[maxT1Idx+mbrg:]))\n",
    "            yield train_indices,test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate_test_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import KFold\n",
    "cvGen = KFold()\n",
    "\n",
    "for i,(train,test) in enumerate(cvGen.split(X=X)):\n",
    "    print(f'Fold {i} | Train: {train} | Test: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvScore(clf,X,y,sample_weight,scoring='neg_log_loss',t1=None,cv=None,cvGen=None, pctEmbargo=None):\n",
    "    if scoring not in ['neg_log_loss','accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    \n",
    "    from sklearn.metrics import log_loss,accuracy_score\n",
    "    # from clfSequential import PurgedKFold\n",
    "\n",
    "    if cvGen is None:\n",
    "        cvGen=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged\n",
    "    score=[]\n",
    "    for train,test in cvGen.split(X=X):\n",
    "        fit=clf.fit(X=X.iloc[train,:],y=y.iloc[train],\n",
    "        sample_weight=sample_weight.iloc[train].values)\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob=fit.predict_proba(X.iloc[test,:])\n",
    "            score_=-log_loss(y.iloc[test],prob, sample_weight=sample_weight.iloc[test].values,labels=clf.classes_)\n",
    "        else:\n",
    "            pred=fit.predict(X.iloc[test,:])\n",
    "            score_=accuracy_score(y.iloc[test],pred,sample_weight = sample_weight.iloc[test].values)\n",
    "        score.append(score_)\n",
    "    return np.array(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
