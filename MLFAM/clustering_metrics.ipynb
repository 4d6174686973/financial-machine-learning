{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Metric: Variation of Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def numBins(nObs,corr=None):\n",
    "    # Optimal number of bins for discretization\n",
    "    if corr is None: # univariate case\n",
    "        z=(8+324*nObs+12*(36*nObs+729*nObs**2)**.5)**(1/3.)\n",
    "        b=round(z/6.+2./(3*z)+1./3)\n",
    "    else: # bivariate case\n",
    "        b=round(2**-.5*(1+(1+24*nObs/(1.-corr**2))**.5)**.5)\n",
    "    return int(b)\n",
    "\n",
    "def varInfo(x,y,norm=False):\n",
    "    # variation of information\n",
    "    bXY=numBins(x.shape[0],corr=np.corrcoef(x,y)[0,1])\n",
    "    \n",
    "    cXY=np.histogram2d(x,y,bXY)[0]\n",
    "    iXY=mutual_info_score(None,None,contingency=cXY)\n",
    "    hX=ss.entropy(np.histogram(x,bXY)[0]) # marginal\n",
    "    hY=ss.entropy(np.histogram(y,bXY)[0]) # marginal\n",
    "    vXY=hX+hY-2*iXY # variation of information\n",
    "    if norm:\n",
    "        hXY=hX+hY-iXY # joint\n",
    "        vXY/=hXY # normalized variation of information\n",
    "    return vXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "def clusterKMeansBase(corr0,maxNumClusters=10,n_init=10):\n",
    "\n",
    "    x, silh = ((1 - corr0.fillna(0)) / 2.) ** .5, pd.Series(dtype=np.float64) # eval observations matrix from corr0\n",
    "\n",
    "    # try different initializations\n",
    "    for init in range(n_init):\n",
    "        \n",
    "         # try different cluster numbers\n",
    "        for i in range(2,maxNumClusters+1):\n",
    "\n",
    "            # perform kmeans\n",
    "            kmeans_ = KMeans(n_clusters=i, n_init=1)\n",
    "            kmeans_ = kmeans_.fit(x)\n",
    "\n",
    "            # compute silhouette score and quality measure\n",
    "            silh_= silhouette_samples(x,kmeans_.labels_)\n",
    "            stat = (silh_.mean() / silh_.std(), silh.mean() / silh.std())\n",
    "            \n",
    "            # keep fit if previous stat was better\n",
    "            if np.isnan(stat[1]) or stat[0]>stat[1]:\n",
    "                silh, kmeans = silh_, kmeans_\n",
    "    \n",
    "    # order results\n",
    "    newIdx=np.argsort(kmeans.labels_)\n",
    "    corr1=corr0.iloc[newIdx] # reorder rows\n",
    "    corr1=corr1.iloc[:,newIdx] # reorder columns\n",
    "\n",
    "    # rename clusters\n",
    "    clstrs = {i:corr0.columns[np.where(kmeans.labels_==i)[0]].tolist() for i in np.unique(kmeans.labels_)} # cluster members\n",
    "    silh = pd.Series(silh, index=x.index)\n",
    "    \n",
    "    return corr1, clstrs, silh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "def makeNewOutputs(corr0,clstrs,clstrs2):\n",
    "    clstrsNew={}\n",
    "\n",
    "    # build lists for clusters to compare\n",
    "    for i in clstrs.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())]=list(clstrs[i])\n",
    "    for i in clstrs2.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())]=list(clstrs2[i])\n",
    "\n",
    "    # build new correlation matrix from clstrsNew\n",
    "    newIdx = [j for i in clstrsNew for j in clstrsNew[i]]\n",
    "    corrNew = corr0.loc[newIdx,newIdx]\n",
    "\n",
    "    # compute distance matrix of new correlation matrix\n",
    "    x = ((1 - corr0.fillna(0)) / 2.) ** .5\n",
    "\n",
    "    # make array for new cluster labels\n",
    "    kmeans_labels = np.zeros(len(x.columns))\n",
    "\n",
    "    # assign labels of x to labels of kmeans_labels\n",
    "    for i in clstrsNew.keys():\n",
    "        idxs = [x.index.get_loc(k) for k in clstrsNew[i]]\n",
    "        kmeans_labels[idxs] = i\n",
    "\n",
    "    # compute silhouette scores within x using kmeans_labels\n",
    "    silhNew = pd.Series(silhouette_samples(x,kmeans_labels), index=x.index)\n",
    "\n",
    "    return corrNew, clstrsNew, silhNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterKMeansTop(corr0,maxNumClusters=None,n_init=10):\n",
    "\n",
    "    if maxNumClusters == None: \n",
    "        maxNumClusters = corr0.shape[1] - 1\n",
    "    \n",
    "    # run base clustering on corr0\n",
    "    corr1, clstrs, silh = clusterKMeansBase(corr0, maxNumClusters=min(maxNumClusters, corr0.shape[1]-1), n_init=n_init)\n",
    "\n",
    "    # get quality score of each cluster from base clustering\n",
    "    clusterTstats = {i:np.mean(silh[clstrs[i]]) / np.std(silh[clstrs[i]]) for i in clstrs.keys()}\n",
    "\n",
    "    # compute mean of quality scores\n",
    "    tStatMean = sum(clusterTstats.values()) / len(clusterTstats)\n",
    "    \n",
    "    # find subset of clusters with quality score less than mean\n",
    "    redoClusters = [i for i in clusterTstats.keys() if clusterTstats[i] < tStatMean]\n",
    "    \n",
    "    if len(redoClusters) <= 1:\n",
    "        return corr1, clstrs, silh # no clusters to redo, return previous base clustering results\n",
    "    else:\n",
    "        # build new correlation matrix from clusters to redo\n",
    "        keysRedo = [j for i in redoClusters for j in clstrs[i]]\n",
    "        corrTmp = corr0.loc[keysRedo,keysRedo]\n",
    "\n",
    "        # get stats of actual clusters to redo\n",
    "        tStatMean = np.mean([clusterTstats[i] for i in redoClusters])\n",
    "\n",
    "        # run top clustering on new correlation matrix (recursive call)\n",
    "        corr2, clstrs2, silh2 = clusterKMeansTop(corrTmp, maxNumClusters=min(maxNumClusters, corrTmp.shape[1]-1), n_init=n_init)\n",
    "    \n",
    "        # Make new outputs, if necessary\n",
    "        corrNew,clstrsNew,silhNew = makeNewOutputs(corr0, {i:clstrs[i] for i in clstrs.keys() if i not in redoClusters}, clstrs2)\n",
    "        \n",
    "        # get new quality scores for redone clusters\n",
    "        newTstatMean = np.mean([np.mean(silhNew[clstrsNew[i]]) / np.std(silhNew[clstrsNew[i]]) for i in clstrsNew.keys()])\n",
    "\n",
    "        if newTstatMean <= tStatMean:\n",
    "            return corr1, clstrs, silh # return previous base clustering results if quality score is worse\n",
    "        else:\n",
    "            return corrNew, clstrsNew, silhNew # return new clustering results if quality score is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classiﬁcation\n",
    "\n",
    "def getTestData(n_features=100 ,n_informative=25, n_redundant=25, n_samples=10000, random_state=0, sigmaStd=.0):\n",
    "    \n",
    "    # generate a random dataset for a classiﬁcation problem\n",
    "    np.random.seed(random_state)\n",
    "    X,y = make_classiﬁcation(n_samples=n_samples, n_features=n_features-n_redundant, n_informative=n_informative, n_redundant=0, shufﬂe=False, random_state=random_state)\n",
    "    \n",
    "    # name the columns\n",
    "    cols = ['I' + str(i) for i in range(n_informative)] # I = Informative\n",
    "    cols += ['N' + str(i) for i in range(n_features - n_informative - n_redundant)] # N = Noise\n",
    "    \n",
    "    # make dataframe\n",
    "    X,y = pd.DataFrame(X, columns=cols),pd.Series(y)\n",
    "    \n",
    "    # choose random informative features to make redundant\n",
    "    i = np.random.choice(range(n_informative), size=n_redundant)\n",
    "\n",
    "    # make inverted dict to print\n",
    "    r_to_i_map = {f\"R{k}\":f\"I{v}\" for k,v in enumerate(i)}  # Ri : Ii\n",
    "    i_to_r_map = {}\n",
    "    for k,v in r_to_i_map.items():\n",
    "        i_to_r_map[v] = i_to_r_map.get(v, []) + [k] # Ii : Ri\n",
    "    print(f\"Informative Features used to generate Redundant Features: \")\n",
    "    for k,v in i_to_r_map.items():\n",
    "        print(f\"{k} : {v}\")\n",
    "\n",
    "    # make redundant features\n",
    "    for k,j in enumerate(i):\n",
    "        X['R' + str(k)] = X['I' + str(j)] + np.random.normal(size = X.shape[0]) * sigmaStd # R = Redundant\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = getTestData(40,5,30,10000,sigmaStd=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Distance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(X.columns)\n",
    "metric = np.full([l,l], np.nan)\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        if not i == j: \n",
    "            metric[i,j] = varInfo(X.iloc[:,i].values, X.iloc[:,j].values, norm=True)\n",
    "        else:\n",
    "            metric[i,j] = 0\n",
    "\n",
    "corr0 = pd.DataFrame(metric, index=X.columns, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_corr(corr0):\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=corr0,\n",
    "            x=corr0.index.astype(str),\n",
    "            y=corr0.columns.astype(str),\n",
    "            colorscale='Viridis'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='VarInfo Matrix',\n",
    "        width=800,\n",
    "        height=800,)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(corr0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Distance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1, clstrs, silh = clusterKMeansTop(corr0=corr0, maxNumClusters=10, n_init=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(corr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster : Features\")\n",
    "for k,v in clstrs.items():\n",
    "    print(k,\":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
