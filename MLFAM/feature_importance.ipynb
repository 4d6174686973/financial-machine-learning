{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassiﬁer\n",
    "from sklearn.ensemble import BaggingClassiﬁer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains a summary of Feature Importance methods with example codes from Machine Learning for Asset Managers by Marcos Lopez de Prado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveats of p-Values:\n",
    "1. Rely on the assumptions\n",
    "    - correct model speciﬁcation\n",
    "    - mutually uncorrelated regressors\n",
    "    - white noise residuals\n",
    "2. For highly multicollinear (mutually correlated) explanatory variables, p-values cannot be robustly estimated\n",
    "    - traditional regression methods cannot discriminate among redundant\n",
    "explanatory variables\n",
    "3. Evaluate a probability that is not entirely relevant\n",
    "    - Given a null hypothesis H0 and an estimated coefﬁcient β, the p-value estimates the probability of obtaining a result equal or more extreme than β, subject to H0 being true\n",
    "    - However, researchers are often more interested in a different probability, namely, the probability of H0 being true, subject to having observed β.\n",
    "    - This probability can be computed using Bayes theorem, alas at the expense of making additional assumptions (Bayesian priors)\n",
    "4. Assesses significance in-sample\n",
    "    - The entire sample is used to solve two tasks: estimating the coefﬁcients and determining their signiﬁcance\n",
    "    - Running multiple in-sample tests on the same data set is likely to produce a false discovery, a practice known as p-hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classiﬁcation\n",
    "\n",
    "def getTestData(n_features=100 ,n_informative=25, n_redundant=25, n_samples=10000, random_state=0, sigmaStd=.0):\n",
    "    \n",
    "    # generate a random dataset for a classiﬁcation problem\n",
    "    np.random.seed(random_state)\n",
    "    X,y = make_classiﬁcation(n_samples=n_samples, n_features=n_features-n_redundant, n_informative=n_informative, n_redundant=0, shufﬂe=False, random_state=random_state)\n",
    "    \n",
    "    # name the columns\n",
    "    cols = ['I' + str(i) for i in range(n_informative)] # I = Informative\n",
    "    cols += ['N' + str(i) for i in range(n_features - n_informative - n_redundant)] # N = Noise\n",
    "    \n",
    "    # make dataframe\n",
    "    X,y = pd.DataFrame(X, columns=cols),pd.Series(y)\n",
    "    \n",
    "    # choose random informative features to make redundant\n",
    "    i = np.random.choice(range(n_informative), size=n_redundant)\n",
    "\n",
    "    # make inverted dict to print\n",
    "    r_to_i_map = {f\"R{k}\":f\"I{v}\" for k,v in enumerate(i)}  # Ri : Ii\n",
    "    i_to_r_map = {}\n",
    "    for k,v in r_to_i_map.items():\n",
    "        i_to_r_map[v] = i_to_r_map.get(v, []) + [k] # Ii : Ri\n",
    "    print(f\"Informative Features used to generate Redundant Features: \")\n",
    "    for k,v in i_to_r_map.items():\n",
    "        print(f\"{k} : {v}\")\n",
    "\n",
    "    # make redundant features\n",
    "    for k,j in enumerate(i):\n",
    "        X['R' + str(k)] = X['I' + str(j)] + np.random.normal(size = X.shape[0]) * sigmaStd # R = Redundant\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.discrete.discrete_model as sm\n",
    "\n",
    "# fit logit model on generated test data and obtain p-values\n",
    "X,y = getTestData(40,5,30,10000,sigmaStd=.1)\n",
    "ols = sm.Logit(y,X).ﬁt(disp=0)\n",
    "pvalues = ols.pvalues.sort_values(ascending=False)\n",
    "\n",
    "# plot the p-values associated with the coefficients\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=pvalues, y=pvalues.index, orientation='h'))\n",
    "fig.update_layout(\n",
    "    title=\"p-values of the coefficients\",\n",
    "    xaxis_title=\"p-values\",\n",
    "    yaxis_title=\"Features\",\n",
    "    height=800,\n",
    "    width=800, \n",
    ")\n",
    "fig.add_vline(x=0.05, line_width=2, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "- Only four out of the thirty-ﬁve nonnoise features are deemed statistically signiﬁcant: I1, R29, R27, I3\n",
    "- Noise features are ranked as relatively important (with positions 9, 11, 14, 18, and 26)\n",
    "- Fourteen of the features ranked as least important are not noise\n",
    "\n",
    "$\\rightarrow$ p-values misrepresent the ground truth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Decrease Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample of size $N$, $F$ features $\\{X_f\\}_{f=1,...,F}$ and one label per observation.\n",
    "- Tree-Based algo splits at each node $t$ its labels into two samples: for given $X_f$ labels in node $t$ associated with a $X_f$ below a threshhold $\\tau$ are placed in left sample and rest in right sample\n",
    "\n",
    "The information gain that results from a split is measured in terms of the resulting reduction in impurity. Here we use Entropy as a measure of impurity but other measures are also possible (e.g. Gini impurity).\n",
    "\n",
    "<u>Impurity Measure: Entropy</u></br>\n",
    "\n",
    "$i[t] = - \\sum_{j=1}^{J} p(j|t) \\log_{2} p(j|t)$\n",
    "\n",
    "- $p(j|t)$ is the proportion of observations of class $j$ at node $t$\n",
    "- $J$ is the number of classes\n",
    "\n",
    "\n",
    "<u>Information Gain</u></br>\n",
    "\n",
    "$\\Delta g[t,f] = i[t] - \\frac{N_{t}^{(0)}}{N_t} i[t^{(0)}] - \\frac{N_{t}^{(1)}}{N_t} i[t^{(1)}]$\n",
    "\n",
    "Where\n",
    "- $i[t]$ is the impurity of labels at node t before split\n",
    "- $i[t^{(0)}]$ is the impurity of labels in the left sample\n",
    "- $i[t^{(1)}]$ is the impurity of labels in the right sample\n",
    "\n",
    "At each node t, the classiﬁcation algorithm evaluates $\\Delta g[t,f]$ for various features in $\\{X_f\\}_{f=1,...,F}$\n",
    "\n",
    "Mathematics Sklearn: https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation\n",
    "\n",
    "<u>Mean Decrease Impurity (MDI)</u></br>\n",
    "\n",
    "The importance of a feature can be computed as the weighted information gain ($\\Delta g[t,f]$) across all nodes where that feature was selected.\n",
    "\n",
    "MDI was introduced by Breimann (2001) Random Forests: https://link.springer.com/article/10.1023/A:1010933404324\n",
    "\n",
    "For algorithms that combine ensembles of trees, like random forests, we can further estimate the mean and variance of MDI values for each feature across all trees. These mean and variance estimates, along with the central limit theorem, are useful in testing the significance of a feature against a user-defined null hypothesis.\n",
    "\n",
    "<u>Bootstrap Aggregation (Bagging)</u></br>\n",
    "\n",
    "1. Draw $B$ bootstrap samples from the training data (random sampling with replacement)\n",
    "2. Train a classiﬁcation tree $T_b$ on each bootstrap sample $b=1,...,B$\n",
    "3. The ensemble forecast is the simple average of the individual forecasts from the $B$ models. In the case of categorical variables, the probability that an observation belongs to a class is given by the proportion of estimators that classify that observation as a member of that class (majority voting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDI(ﬁt,featNames):\n",
    "    # feat importance based on IS mean impurity reduction\n",
    "\n",
    "    # get importances for each tree\n",
    "    df0 = {i: tree.feature_importances_ for i,tree in enumerate(ﬁt.estimators_)}\n",
    "\n",
    "    # convert to DF\n",
    "    df0 = pd.DataFrame.from_dict(df0, orient='index')\n",
    "    df0.columns = featNames\n",
    "    df0 = df0.replace(0, np.nan) # because max_features=1\n",
    "    \n",
    "    # compute mean and std of importances\n",
    "    imp = pd.concat({'mean':df0.mean(), 'std':df0.std()*df0.shape[0]**-.5}, axis=1) # CLT\n",
    "    imp /= imp['mean'].sum()\n",
    "\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = getTestData(40, 5, 30, 10000, sigmaStd=.1)\n",
    "\n",
    "clf = DecisionTreeClassiﬁer(\n",
    "    criterion='entropy',\n",
    "    max_features=1,\n",
    "    class_weight='balanced',\n",
    "    min_weight_fraction_leaf=0\n",
    "    )\n",
    "\n",
    "clf = BaggingClassiﬁer(\n",
    "    estimator=clf,\n",
    "    n_estimators=1000,\n",
    "    max_features=1.,\n",
    "    max_samples=1.,\n",
    "    oob_score=False\n",
    "    )\n",
    "\n",
    "ﬁt = clf.ﬁt(X,y)\n",
    "imp = featImpMDI(ﬁt, featNames=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "imp_sorted = imp.copy()\n",
    "imp_sorted.sort_values(by='mean', ascending=True, inplace=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=imp_sorted['mean'],\n",
    "                     y=imp_sorted.index,\n",
    "                     error_x=dict(\n",
    "                         type='data',\n",
    "                         symmetric=True,\n",
    "                         array=imp_sorted['std']\n",
    "                         ),\n",
    "                     orientation='h'))\n",
    "fig.update_layout(\n",
    "    title=\"MDI Results\",\n",
    "    xaxis_title=\"MDI mean with standard deviation\",\n",
    "    yaxis_title=\"Features\",\n",
    "    height=800,\n",
    "    width=800, \n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "- All nonnoisy features (I, R) are ranked higher than noisy features.\n",
    "- Still, a small number of nonnoisy features appear to be much more important than their peer\n",
    "\n",
    "Out of the four caveats of p-values, the MDI method deals with three:\n",
    "1. MDI’s computational nature circumvents the need for strong distributional assumptions that could be false - we are not imposing a particular tree structure or algebraic specification, or relying on stochastic or distributional characteristics of residuals.\n",
    "2. Whereas betas are estimated on a single sample, ensemble MDIs are derived from a bootstrap of trees. Accordingly, the variance of MDI estimates can be reduced by increasing the number of trees in ensemble methods in general, or in a random forest in particular. This reduces the probability of false positives caused by overfitting. Also, unlike p-values, MDI’s estimation does not require the inversion of a possibly ill-conditioned matrix.\n",
    "3. The goal of the tree-based classifiers is not to estimate the coefficients of a given algebraic equation, thus estimating the probability of a particular null hypothesis is irrelevant. In other words, MDI corrects for caveat 3 by finding the important features in general, irrespective of any particular parametric specification.\n",
    "\n",
    "However, MDI does not deal with the fourth caveat:\n",
    "\n",
    "4. The procedure itself does not involve cross-validation. Therefore, the one caveat of p-values that MDI does not fully solve is that MDI is also computed in-sample. To confront this final caveat, we need to introduce the concept of mean-decrease accuracy.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Decrease Accuracy / Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit a model and compute cross-validated performance\n",
    "2. Compute cross-validated performance for shuffled observations associated with one of the features -> this gives modified coss validated performance per feature\n",
    "3. Derive MDA associated with a particular feature by comparing cross-validated performance before and after shuffling. For important features there should be a significant decay in performance after shuffling.\n",
    "\n",
    "<u>Important:</u>\n",
    "\n",
    "When features are not independent, MDA may underestimate the importance of interrelated features. At the extreme, given two highly important but identical features, MDA may conclude that both features are relatively unimportant, because the effect of shuffling one may be partially compensated by not shuffling the other.\n",
    "\n",
    "Despite it's name accuracy may not be a good choice to evaluate the cross-validated performance in case of Finance because accuracy scores a classifier in terms of its proportion of correct predictions, but in Finance we are more interested in the magnitude of the prediction error. A classifier may achieve high accuracy even though it made good predictions with low confidence and bad predictions with high confidence.\n",
    "\n",
    "<u> Negative average likelihood </u>\n",
    "\n",
    "Good alternative to accuracy is log-loss (cross-entropy loss). Log-loss scores a classifier in terms of average log-likelihood of the true labels but are not easy to interpret and compare so better use negative average likelihood (NegAL).\n",
    "\n",
    "$$NegAL = - N^{-1} \\sum_{n=0}^{N-1} \\sum_{k=0}^{K-1} y_{n,k} p_{n,k}$$\n",
    "\n",
    "Where\n",
    "- $p_{n,k}$ is the probability associated with prediction n of label k\n",
    "- $y_{n,k}$ is the indicator function $y_{n,k} \\in \\{0,1\\}$ where $y_{n,k}=1$ when observation n was assigned label k and $y_{n,k}=0$ otherwise\n",
    "\n",
    "<u> Probability-weighted accuracy </u>\n",
    "\n",
    "Another alternative to accuracy is probability-weighted accuracy (PWA). PWA is the average probability associated with the true labels.\n",
    "\n",
    "$$PWA = \\frac{\\sum_{n=0}^{N-1} y_n (p_n - K^{-1})}{\\sum_{n=0}^{N-1}(p_n - K^{-1})}$$\n",
    "\n",
    "Where\n",
    "- $p_n = max_k\\{p_{n,k}\\}$\n",
    "- $y_n$ is the indicator function $y_n \\in \\{0,1\\}$ where $y_n=1$ when the prediction was correct and $y_n=0$ otherwise\n",
    "\n",
    "PWA punishes bad predictions made with high confidence more severely than accuracy, but less severely than log-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y1 = pd.Series\n",
    "\n",
    "# prob = [[0.826 0.174]\n",
    "#  [0.814 0.186]\n",
    "#  [0.813 0.187]\n",
    "#  ...\n",
    "#  [0.971 0.029]\n",
    "#  [0.892 0.108]\n",
    "#  [0.931 0.069]]\n",
    "\n",
    "# classes = ['0', '1']\n",
    "\n",
    "# -log_loss = float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code for pwa\n",
    "\n",
    "y_true = pd.Series([0, 1, 0, 0, 1, 0, 1, 1, 1, 0])\n",
    "y_prob = pd.Series([0.1, 0.9, 0.2, 0.3, 0.8, 0.1, 0.9, 0.9, 0.9, 0.1])\n",
    "y_pred = pd.Series([0, 1, 0, 1, 1, 0, 1, 1, 1, 0])\n",
    "labels = ['0', '1']\n",
    "\n",
    "y_indicator = y_true.eq(y_pred).astype(int)\n",
    "y_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pwa(y_true, y_pred, y_prob, labels):\n",
    "    \n",
    "    y_indicator = y_true.eq(y_pred).astype(int)\n",
    "    \n",
    "    # ... to be completed ...\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection._split import KFold\n",
    "\n",
    "def featImpMDA(clf,X,y,n_splits=10):\n",
    "    # feat importance based on OOS score reduction\n",
    "\n",
    "    cvGen = KFold(n_splits=n_splits)\n",
    "    scr0, scr1 = pd.Series(dtype=float), pd.DataFrame(columns=X.columns)\n",
    "\n",
    "    for i,(train,test) in enumerate(cvGen.split(X=X)):\n",
    "        X0, y0 = X.iloc[train,:], y.iloc[train] # get train set\n",
    "        X1, y1 = X.iloc[test,:], y.iloc[test] # get test set\n",
    "        fit = clf.fit(X=X0, y=y0) # the fit occurs here\n",
    "        prob = fit.predict_proba(X1) # prediction before shuffling\n",
    "        pred = fit.predict(X1) # prediction before shuffling\n",
    "\n",
    "        # compute logloss before shuffling\n",
    "        scr0.loc[i] = -log_loss(y1, prob, labels=clf.classes_)\n",
    "        \n",
    "        for j in X.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values) # shuffle one column\n",
    "            prob = fit.predict_proba(X1_) # prediction after shuffling\n",
    "\n",
    "            # compute logloss after shuffling\n",
    "            scr1.loc[i,j] = -log_loss(y1, prob, labels=clf.classes_)\n",
    "    \n",
    "    # compute importance for each feature\n",
    "    imp = (-1 * scr1).add(scr0, axis=0)\n",
    "    imp = imp / (-1 * scr1) # normalize\n",
    "    imp = pd.concat({'mean' : imp.mean(), 'std' : imp.std() * imp.shape[0] ** -.5}, axis=1) # CLT\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = getTestData(40,5,30,10000,sigmaStd=.1)\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features=1,\n",
    "    class_weight = 'balanced',\n",
    "    min_weight_fraction_leaf=0\n",
    "    )\n",
    "\n",
    "clf = BaggingClassifier(\n",
    "    estimator=clf,\n",
    "    n_estimators=1000,\n",
    "    max_features = 1.,\n",
    "    max_samples=1.,\n",
    "    oob_score=False\n",
    "    )\n",
    "\n",
    "imp = featImpMDA(clf, X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "imp_sorted = imp.copy()\n",
    "imp_sorted.sort_values(by='mean', ascending=True, inplace=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=imp_sorted['mean'],\n",
    "                     y=imp_sorted.index,\n",
    "                     error_x=dict(\n",
    "                         type='data',\n",
    "                         symmetric=True,\n",
    "                         array=imp_sorted['std']\n",
    "                         ),\n",
    "                     orientation='h'))\n",
    "fig.update_layout(\n",
    "    title=\"MDA Results\",\n",
    "    xaxis_title=\"MDA mean with standard deviation\",\n",
    "    yaxis_title=\"Features\",\n",
    "    height=800,\n",
    "    width=800, \n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Feature Importance (CFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFI is a method to deal with substitution effects.\n",
    "\n",
    "Substitution effects arise when two features share predictive information and thus can bias the results from feature importance methods. In the case of MDI, the importance of two identical features will be halved, as they are randomly chosen with equal probability. In the case of MDA, two identical features may be considered relatively unimportant, even if they are critical, because the effect of shuffling one may be compensated by the other.\n",
    "\n",
    "CFI involves two steps:\n",
    "1. Finding the number and constituents of the clusters of features\n",
    "2. Applying the feature importance analysis on groups of similar features rather than on individual features\n",
    "\n",
    "<u> Step 1: Features Clustering</u>\n",
    "- Project the observed features into a metric space, resulting in a matrix $\\{X_f\\}_{f=1,...,F}$\n",
    "- Use correlation based approach or information theoretic distance metrics to cluster the features\n",
    "- Information theoretic metrics have advantage that they recognizing redundant features that are the result of nonlinear combinations of informative features\n",
    "- Apply ONC algorithm (optimal number of clusters)\n",
    "\n",
    "\n",
    "Some silhouette scores may be low due one feature being a combination of multiple features across clusters. This is a problem, because ONC cannot assign one feature to multiple clusters. In this case, the following transformation may help reduce the multicollinearity of the system.\n",
    "\n",
    "\n",
    "Replace features included in that cluster with residual features outside of cluster k. \n",
    "- $D_k$ subset of index features $D={1,...,F}$ included in k\n",
    "- For given $X_{n,i} = \\alpha_i + \\sum_{j \\in \\{\\cup _{l<k} D_l \\}} \\beta_{i,j} X_{n,j} + \\epsilon_{n, i}$ where n is the index of observations per feature\n",
    "- If degrees of freedom in the above regression is too low, one option is to use as regressors linear combinations of the features within each cluster\n",
    "- One of the properties of OLS residuals is that they are orthogonal to the regressors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Clustering</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "def clusterKMeansBase(corr0,maxNumClusters=10,n_init=10):\n",
    "    x,silh=((1-corr0.fillna(0))/2.)**.5,pd.Series(dtype=np.float64)# observations matrix\n",
    "    \n",
    "    for init in range(n_init):\n",
    "        \n",
    "        for i in range(2,maxNumClusters+1):\n",
    "            kmeans_=KMeans(n_clusters=i,n_init=1)\n",
    "            kmeans_=kmeans_.fit(x)\n",
    "            silh_=silhouette_samples(x,kmeans_.labels_)\n",
    "            stat=(silh_.mean()/silh_.std(),silh.mean()/silh.std())\n",
    "            \n",
    "            if np.isnan(stat[1]) or stat[0]>stat[1]:\n",
    "                silh,kmeans=silh_,kmeans_\n",
    "    \n",
    "    newIdx=np.argsort(kmeans.labels_)\n",
    "    corr1=corr0.iloc[newIdx] # reorder rows\n",
    "    corr1=corr1.iloc[:,newIdx] # reorder columns\n",
    "\n",
    "    clstrs={i:corr0.columns[np.where(kmeans.labels_==i)[0]].tolist() for i in np.unique(kmeans.labels_)} # cluster members\n",
    "    silh=pd.Series(silh,index=x.index)\n",
    "    \n",
    "    return corr1,clstrs,silh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "def makeNewOutputs(corr0,clstrs,clstrs2):\n",
    "    clstrsNew={}\n",
    "\n",
    "    for i in clstrs.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())]=list(clstrs[i])\n",
    "\n",
    "    for i in clstrs2.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())]=list(clstrs2[i])\n",
    "\n",
    "    newIdx = [j for i in clstrsNew for j in clstrsNew[i]]\n",
    "    corrNew = corr0.loc[newIdx,newIdx]\n",
    "    x = ((1 - corr0.fillna(0)) / 2.) ** .5\n",
    "    kmeans_labels = np.zeros(len(x.columns))\n",
    "\n",
    "    for i in clstrsNew.keys():\n",
    "        idxs=[x.index.get_loc(k) for k in clstrsNew[i]]\n",
    "        kmeans_labels[idxs]=i\n",
    "\n",
    "    silhNew=pd.Series(silhouette_samples(x,kmeans_labels),index=x.index)\n",
    "\n",
    "    return corrNew,clstrsNew,silhNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterKMeansTop(corr0,maxNumClusters=None,n_init=10):\n",
    "\n",
    "    if maxNumClusters==None:maxNumClusters=corr0.shape[1]-1\n",
    "    \n",
    "    corr1, clstrs, silh = clusterKMeansBase(corr0, maxNumClusters=min(maxNumClusters, corr0.shape[1]-1), n_init=n_init)\n",
    "\n",
    "    clusterTstats = {i:np.mean(silh[clstrs[i]]) / np.std(silh[clstrs[i]]) for i in clstrs.keys()}\n",
    "\n",
    "    tStatMean = sum(clusterTstats.values())/len(clusterTstats)\n",
    "    \n",
    "    redoClusters = [i for i in clusterTstats.keys() if clusterTstats[i] < tStatMean]\n",
    "    \n",
    "    if len(redoClusters)<=1:\n",
    "        return corr1,clstrs,silh\n",
    "    else:\n",
    "        keysRedo = [j for i in redoClusters for j in clstrs[i]]\n",
    "        corrTmp = corr0.loc[keysRedo,keysRedo]\n",
    "        tStatMean = np.mean([clusterTstats[i] for i in redoClusters])\n",
    "        corr2, clstrs2, silh2 = clusterKMeansTop(corrTmp, maxNumClusters=min(maxNumClusters, corrTmp.shape[1]-1), n_init=n_init)\n",
    "    \n",
    "        # Make new outputs, if necessary\n",
    "        corrNew,clstrsNew,silhNew = makeNewOutputs(corr0, {i:clstrs[i] for i in clstrs.keys() if i not in redoClusters}, clstrs2)\n",
    "        newTstatMean = np.mean([np.mean(silhNew[clstrsNew[i]]) / np.std(silhNew[clstrsNew[i]]) for i in clstrsNew.keys()])\n",
    "\n",
    "        if newTstatMean <= tStatMean:\n",
    "            return corr1, clstrs, silh\n",
    "        else:\n",
    "            return corrNew,clstrsNew,silhNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Clustered MDI</u>\n",
    "\n",
    "\n",
    "We compute the clustered MDI as the sum of the MDI values of the features that constitute that cluster. If there is one feature per cluster, then MDI and clustered MDI are the same. In the case of an ensemble of trees, there is one clustered MDI for each tree, which allows us to compute the mean clustered MDI, and standard deviation around the mean clustered MDI, similarly to how we did for the feature MDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupMeanStd(df0,clstrs):\n",
    "    out = pd.DataFrame(columns=['mean','std'])\n",
    "    for i, j in clstrs.items():\n",
    "        df1 = df0[j].sum(axis=1)  # sum of each MDI in the cluster\n",
    "        out.loc['C_'+str(i), 'mean'] = df1.mean()  # mean \n",
    "        out.loc['C_'+str(i), 'std'] = df1.std() * df1.shape[0] ** -.5  # std * sqrt(n)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDI_Clustered(fit, featNames, clstrs):\n",
    "   \n",
    "    # get importances of each tree\n",
    "    df0 = {i:tree.feature_importances_ for i,tree in enumerate(fit.estimators_)}\n",
    "\n",
    "    # convert to dataframe\n",
    "    df0 = pd.DataFrame.from_dict(df0,orient='index')\n",
    "    df0.columns = featNames\n",
    "    df0 = df0.replace(0,np.nan) # because max_features=1\n",
    "\n",
    "    # get mean and std\n",
    "    imp = groupMeanStd(df0,clstrs)\n",
    "    imp /= imp['mean'].sum()\n",
    "\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Clustered MDA</u>\n",
    "\n",
    "When computing clustered MDA, instead of shuffling one feature at a time, we shuffle all of the features that constitute a given cluster. If there is one cluster per feature, then MDA and clustered MDA are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDA_Clustered(clf,X,y,clstrs,n_splits=10):\n",
    "    from sklearn.metrics import log_loss\n",
    "    from sklearn.model_selection._split import KFold\n",
    "    \n",
    "    cvGen = KFold(n_splits=n_splits)\n",
    "    scr0, scr1 = pd.Series(dtype=np.float64), pd.DataFrame(columns=clstrs.keys())  # make empty scrorer\n",
    "    \n",
    "    for i,(train,test) in enumerate(cvGen.split(X=X)):\n",
    "\n",
    "        # train and test by cv folds\n",
    "        X0, y0 = X.iloc[train,:], y.iloc[train]\n",
    "        X1, y1 = X.iloc[test,:], y.iloc[test]\n",
    "\n",
    "        # fit classifier and compute score\n",
    "        fit = clf.fit(X=X0,y=y0)\n",
    "        prob = fit.predict_proba(X1)\n",
    "        scr0.loc[i] = -log_loss(y1, prob, labels=clf.classes_)\n",
    "\n",
    "        for j in scr1.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            \n",
    "            # shuffle cluster\n",
    "            for k in clstrs[j]:\n",
    "                np.random.shuffle(X1_[k].values) \n",
    "\n",
    "            # fit and compute score after 1 cluster shuffled\n",
    "            prob = fit.predict_proba(X1_)\n",
    "            scr1.loc[i,j] = -log_loss(y1, prob, labels=clf.classes_)\n",
    "\n",
    "    # compute importances as difference between scores\n",
    "    imp = (-1 * scr1).add(scr0, axis=0)\n",
    "    imp = imp / (-1*scr1)\n",
    "\n",
    "    # mean and std\n",
    "    imp = pd.concat({'mean' : imp.mean(),'std' : imp.std() * imp.shape[0] ** -.5}, axis=1)\n",
    "    imp.index = ['C_'+str(i) for i in imp.index]\n",
    "    \n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Features Clustering</u>\n",
    "\n",
    "In a nonexperimental setting, the researcher should denoise and detone the correlation matrix before clustering, as explained in Section 2. We do not do so in this experiment as a matter of testing the robustness of the method (results are expected to be better on a denoised and detoned correlation matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=getTestData(40,5,30,10000,sigmaStd=.1)\n",
    "# corr0,clstrs,silh = clusterKMeansBase(X.corr(),maxNumClusters=10,n_init=10)\n",
    "corr0,clstrs,silh = clusterKMeansTop(X.corr(),maxNumClusters=10,n_init=10)\n",
    "\n",
    "# sns.heatmap(corr0,cmap='viridis')\n",
    "\n",
    "# plot heatmap with plotly go figure\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=corr0,\n",
    "        x=corr0.index,\n",
    "        y=corr0.columns,\n",
    "        colorscale='Viridis')\n",
    "        )\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix',\n",
    "    xaxis_nticks=36,\n",
    "    width=800,\n",
    "    height=800,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Results from Clustering</u>\n",
    "\n",
    "ONC correctly recognizes that there are six relevant clusters (one cluster for each informative feature, plus one cluster of noise features), and it assigns the redundant features to the cluster that contains the informative feature from which the redundant features were derived. Given the low correlation across clusters, there is no need to replace the features with their residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Apply Clustered MDI</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier(criterion='entropy',max_features=1, class_weight='balanced', min_weight_fraction_leaf=0)\n",
    "clf=BaggingClassifier(estimator=clf,n_estimators=1000,max_features=1.,max_samples=1.,oob_score=False)\n",
    "fit=clf.fit(X,y)\n",
    "imp=featImpMDI_Clustered(fit,X.columns,clstrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "imp_sorted = imp.copy()\n",
    "imp_sorted.sort_values(by='mean', ascending=True, inplace=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=imp_sorted['mean'],\n",
    "                     y=imp_sorted.index,\n",
    "                     error_x=dict(\n",
    "                         type='data',\n",
    "                         symmetric=True,\n",
    "                         array=imp_sorted['std']\n",
    "                         ),\n",
    "                     orientation='h'))\n",
    "fig.update_layout(\n",
    "    title=\"Clustered MDI\",\n",
    "    xaxis_title=\"MDI mean with standard deviation\",\n",
    "    yaxis_title=\"Features\",\n",
    "    height=400,\n",
    "    width=400, \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Results from Clustered MDI</u>\n",
    "\n",
    "- Noise Features are all in cluster C5 and are the least important\n",
    "- C5 haa at least half importance than the others: clustered MDI works better than non-clustered MDI\n",
    "- Shows different importances for non-noise features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Apply Clustered MDA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier(criterion='entropy',max_features=1,class_weight='balanced',min_weight_fraction_leaf=0)\n",
    "clf=BaggingClassifier(estimator=clf,n_estimators=1000,max_features=1.,max_samples=1.,oob_score=False)\n",
    "imp=featImpMDA_Clustered(clf,X,y,clstrs,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "imp_sorted = imp.copy()\n",
    "imp_sorted.sort_values(by='mean', ascending=True, inplace=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=imp_sorted['mean'],\n",
    "                     y=imp_sorted.index,\n",
    "                     error_x=dict(\n",
    "                         type='data',\n",
    "                         symmetric=True,\n",
    "                         array=imp_sorted['std']\n",
    "                         ),\n",
    "                     orientation='h'))\n",
    "fig.update_layout(\n",
    "    title=\"Clustered MDI\",\n",
    "    xaxis_title=\"MDI mean with standard deviation\",\n",
    "    yaxis_title=\"Features\",\n",
    "    height=400,\n",
    "    width=400, \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Results from Clustered MDA</u>\n",
    "\n",
    "- C5 has essentially zero importance -> can be discarded as irrelevant\n",
    "- all other clusters have very similar importance -> contrast to non-clustered MDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers (from AIFML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to tackle overfitting:\n",
    "\n",
    "1. Set a parameter max_features to a lower value, as a way of forcing discrepancy between trees.\n",
    "2. Early stopping: Set the regularization parameter *min_weight_fraction_leaf* to a sufficiently large value (e.g., 5%) such that out-of-bag accuracy converges to out-of-sample (k-fold) accuracy.\n",
    "3. Use *BaggingClassifier* on *DecisionTreeClassifier* where *max_samples* is set to the average uniqueness (avgU) between samples.\n",
    "4. Use *BaggingClassifier* on *RandomForestClassifier* where *max_samples* is set to the average uniqueness (avgU) between samples.\n",
    "5. Modify the RF class to replace standard bootstrapping with sequential bootstrapping\n",
    "\n",
    "There are three ways of setting up an RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgU = 0.5 # see sample weights notebook for the functions\n",
    "\n",
    "# 1\n",
    "clf0 = RandomForestClassifier(n_estimators=1000, class_weight='balanced_subsample', criterion='entropy')\n",
    "\n",
    "# 2\n",
    "clf1 = DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced')\n",
    "clf1 = BaggingClassifier(estimator=clf1, n_estimators=1000, max_samples=avgU)\n",
    "\n",
    "# 3\n",
    "clf2 = RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample')\n",
    "clf2 = BaggingClassifier(estimator=clf2, n_estimators=1000, max_samples=avgU, max_features=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author suggested to fit RF on PCA of the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
