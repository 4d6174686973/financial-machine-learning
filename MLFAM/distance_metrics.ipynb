{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite all of its virtues, correlation (and covariance matrices) suffers from some critical limitations as a measure of codependence.\n",
    "\n",
    "How do we overcome this limitations?\n",
    "- information theory in general, and the concept of Shannonâ€™s entropy in particular, also have useful applications in finance.\n",
    "- quantify the amount of uncertainty associated with a random variable\n",
    "- Information theory is also essential to ML, because the primary goal of many ML algorithms is to reduce the amount of uncertainty involved in the solution to a problem.\n",
    "\n",
    "The explained distance metrics are useful for:\n",
    "1. defining the objective function in decision tree learning\n",
    "2. defining the loss function for classification problems\n",
    "3. evaluating the distance between two random variables\n",
    "4. comparing clusters\n",
    "5. feature selection\n",
    "\n",
    "For the math we refer to the book: Machine Learning for Asset Managers by Marcos Lopez de Prado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal, Joint, Conditional Entropies, and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np,scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "x = np.random.normal(0,1,1000)\n",
    "y = np.random.normal(0,1,1000)\n",
    "bins = 100\n",
    "cXY = np.histogram2d(x,y,bins)[0]\n",
    "\n",
    "# entropy\n",
    "hX = ss.entropy(np.histogram(x,bins)[0]) # marginal x\n",
    "hY = ss.entropy(np.histogram(y,bins)[0]) # marginal y\n",
    "\n",
    "iXY = mutual_info_score(None,None,contingency=cXY) # mutual information\n",
    "iXYn = iXY / min(hX,hY) # normalized mutual information\n",
    "\n",
    "hXY = hX + hY - iXY # joint entropy\n",
    "\n",
    "hX_Y = hXY - hY # conditional xy\n",
    "hY_X = hXY - hX # conditional yx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('H(X): \\t\\t\\t %.3f bits' % hX, \"\\t Marginal Entropy\")\n",
    "print('H(Y): \\t\\t\\t %.3f bits' % hY, \"\\t Marginal Entropy\")\n",
    "print('H(X,Y): \\t\\t %.3f bits' % hXY, \"\\t Joint Entropy\")\n",
    "print('H(X|Y): \\t\\t %.3f bits' % hX_Y, \"\\t Conditional Entropy\")\n",
    "print('H(Y|X): \\t\\t %.3f bits' % hY_X, \"\\t Conditional Entropy\")\n",
    "print('I(X,Y): \\t\\t %.3f bits' % iXY, \"\\t Mutual Information\")\n",
    "print('I(X,Y)/min(H(X),H(Y)): \\t %.3f' % iXYn, \"\\t\\t Normalized Mutual Information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information, Variation of Information, and normalized variation of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np,scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "#- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "def varInfo(x,y,bins,norm=False):\n",
    "    # variation of information\n",
    "    cXY=np.histogram2d(x,y,bins)[0]\n",
    "    iXY=mutual_info_score(None,None,contingency=cXY)\n",
    "    hX=ss.entropy(np.histogram(x,bins)[0]) # marginal\n",
    "    hY=ss.entropy(np.histogram(y,bins)[0]) # marginal\n",
    "    vXY=hX+hY-2*iXY # variation of information\n",
    "    if norm:\n",
    "        hXY=hX+hY-iXY # joint\n",
    "        vXY/=hXY # normalized variation of information\n",
    "    return vXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('VI(X,Y): \\t %.3f bits' % varInfo(x,y,bins), \"\\t Variation of Information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation of Information on Discretized Continuous Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Variation of Information so that it now incorporates the optimal binning derived by function numBins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numBins(nObs,corr=None):\n",
    "    # Optimal number of bins for discretization\n",
    "    if corr is None: # univariate case\n",
    "        z=(8+324*nObs+12*(36*nObs+729*nObs**2)**.5)**(1/3.)\n",
    "        b=round(z/6.+2./(3*z)+1./3)\n",
    "    else: # bivariate case\n",
    "        b=round(2**-.5*(1+(1+24*nObs/(1.-corr**2))**.5)**.5)\n",
    "    return int(b)\n",
    "\n",
    "def varInfo(x,y,norm=False):\n",
    "    # variation of information\n",
    "    bXY=numBins(x.shape[0],corr=np.corrcoef(x,y)[0,1])\n",
    "    cXY=np.histogram2d(x,y,bXY)[0]\n",
    "    iXY=mutual_info_score(None,None,contingency=cXY)\n",
    "    hX=ss.entropy(np.histogram(x,bXY)[0]) # marginal\n",
    "    hY=ss.entropy(np.histogram(y,bXY)[0]) # marginal\n",
    "    vXY=hX+hY-2*iXY # variation of information\n",
    "    if norm:\n",
    "        hXY=hX+hY-iXY # joint\n",
    "        vXY/=hXY # normalized variation of information\n",
    "    return vXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('VI(X,Y): \\t %.3f bits' % varInfo(x,y), \"\\t Modified Variation of Information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Normalizzed Mutual Information  of Two Independent Gaussian Random Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualInfo(x,y,norm=False):\n",
    "    # mutual information\n",
    "    bXY=numBins(x.shape[0],corr=np.corrcoef(x,y)[0,1])\n",
    "    cXY=np.histogram2d(x,y,bXY)[0]\n",
    "    iXY=mutual_info_score(None,None,contingency=cXY)\n",
    "    if norm:\n",
    "        hX=ss.entropy(np.histogram(x,bXY)[0]) # marginal\n",
    "        hY=ss.entropy(np.histogram(y,bXY)[0]) # marginal\n",
    "        iXY/=min(hX,hY) # normalized mutual information\n",
    "    return iXY\n",
    "\n",
    "size,seed=5000,0\n",
    "np.random.seed(seed)\n",
    "x=np.random.normal(size=size)\n",
    "e=np.random.normal(size=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "y=0*x+e\n",
    "nmi=mutualInfo(x,y,True)\n",
    "corr=np.corrcoef(x,y)[0,1]\n",
    "\n",
    "fig=go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x,y=y,mode='markers',marker=dict(color='black',size=3)))\n",
    "fig.update_layout(title='No Relationship (Corr: %.3f, NMI: %.3f)' % (corr,nmi),height=600,width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Correlation and Normalized Mutial Information (NMI) are close to 0 so we conclude the two random variables are unrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "y=100*x+e\n",
    "nmi=mutualInfo(x,y,True)\n",
    "corr=np.corrcoef(x,y)[0,1]\n",
    "\n",
    "fig=go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x,y=y,mode='markers',marker=dict(color='black',size=3)))\n",
    "fig.update_layout(title='Linear Relationship (Corr: %.3f, NMI: %.3f)' % (corr,nmi),height=600,width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Correlation is 1 but NMI is approx 0.9 so NMI is more sensitive to the degree of uncertainity associated with e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "y=100*abs(x)+e\n",
    "nmi=mutualInfo(x,y,True)\n",
    "corr=np.corrcoef(x,y)[0,1]\n",
    "\n",
    "fig=go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x,y=y,mode='markers',marker=dict(color='black',size=3)))\n",
    "fig.update_layout(title='Nonlinear Relationship (Corr: %.3f, NMI: %.3f)' % (corr,nmi),height=600,width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: There clearly exists a strong relationship between the two random variables, but the correlation is still near 0 and thus fails to recognize the strong relationship. NMI is not one but significantly higher than 0 and thus recognizes that there is a substantial amount of information shared between the two random variables. In Fact it is not 1 because there are two alternative values of x associated with each value of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
